{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01da353c-38da-432a-83cb-9a13565641bc",
   "metadata": {},
   "source": [
    "# **USE FOR RNA DATA WITH UMIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666f300-3eef-44aa-a202-e79c172d9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm \n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import sem, zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90106a60-f3ec-4c79-babb-7750d443f64e",
   "metadata": {},
   "source": [
    "## **1. Decode Pre-Processed Step 1 Map**\n",
    "- the step1 map is a csv file that associates barcodes to tiles from inital plasmid pool sequencing\n",
    "- step1 map should be processed already (barcodes matching to muliple other barcodes and/or tiles beyond threshold and minimum read thresholds removed)\n",
    "- inital step1 graphs will show you read count distributions to make sure you are happy with thresholds before proceeding \n",
    "- this first block will associate the gene, mutation, and protein sequence information from the origional tile design table with the step1 map containing barcodes based on matching tile DNA sequence\n",
    "- you can skip to the second code block if you already have step1 decoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118c837-405b-48cb-bbb9-7c168116dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for Step1 decoding \n",
    "Lib_Name = 'TL4_test' #UPDATE name you want associated with all file naming \n",
    "step1_initial = 'TL4S1_TL4S1_czb_Min_map4_unique_cat_map4_90_match_10_read_min_filter.csv' #UPDATE processed step1 seq bc lookup table \n",
    "gene_lut = 'Lib4_Tile_LUT.csv ' #UPDATE table used to create varients that associates a DNA sequence with a specific gene/mutations\n",
    "\n",
    "out_s1 = f'{Lib_Name}_decoding_S1'\n",
    "os.makedirs(out_s1, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a597f3-cf48-49c2-8830-aef30606d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of step1 map read coverage to ensure happy to move forward \n",
    "def analyze_cat_counts(csv_file, column, name, count_col=\"Cat_Count\"):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    if column not in df.columns or count_col not in df.columns:\n",
    "        raise ValueError(f\"Required columns '{column}' and/or '{count_col}' not found in {csv_file}\")\n",
    "\n",
    "    # Histograms (normal + log scale)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[count_col], bins=50, kde=False)\n",
    "    plt.title(f\"Histogram of {name} {count_col} (Normal Scale)\")\n",
    "    plt.xlabel(count_col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[count_col], bins=50, kde=False, log_scale=(True, True))\n",
    "    plt.title(f\"Histogram of {name} {count_col} (Log Scale)\")\n",
    "    plt.xlabel(count_col)\n",
    "    plt.ylabel(\"Frequency (log)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Threshold table\n",
    "    thresholds = [1, 5, 10, 25, 50, 100, 1000]\n",
    "    threshold_counts = {t: (df[count_col] >= t).sum() for t in thresholds}\n",
    "    threshold_df = pd.DataFrame.from_dict(threshold_counts, orient=\"index\", columns=[\"Rows ≥ Threshold\"])\n",
    "\n",
    "    # Summary statistics\n",
    "    stats_summary = {\n",
    "        \"min\": int(df[count_col].min()),\n",
    "        \"max\": int(df[count_col].max()),\n",
    "        \"median\": int(df[count_col].median()),\n",
    "        \"mean\": round(df[count_col].mean())\n",
    "    }\n",
    "\n",
    "    # Unique counts in key columns\n",
    "    key_columns = ['Cat', 'HAR', 'HA', 'RTBC', 'HawkBCs', 'ADBC2', 'Designed'] #UPDATE if you have different columns \n",
    "    unique_counts = {}\n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            unique_counts[col] = df[col].nunique()\n",
    "        else:\n",
    "            unique_counts[col] = 'Column not found'\n",
    "\n",
    "    print(f\"\\n{name} Threshold Counts:\")\n",
    "    display(threshold_df)\n",
    "\n",
    "    print(f\"\\n{name} Summary Statistics for {count_col}:\")\n",
    "    for k, v in stats_summary.items():\n",
    "        print(f\"{k.capitalize()}: {v}\")\n",
    "\n",
    "    print(f\"\\n{name} Unique Counts in Key Columns (pre-threshold):\")\n",
    "    for col, count in unique_counts.items():\n",
    "        print(f\"{col}: {count}\")\n",
    "\n",
    "    return threshold_df, stats_summary, unique_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95225f2b-4981-4c31-8da5-8fda1ea743ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute step1 analysis function \n",
    "thresholds, stats, unique_count = analyze_cat_counts(step1_initial, column='Cat',name=f'{Lib_Name}_Step1_Initial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281e939-ddac-4e67-9c17-6742f7a70897",
   "metadata": {},
   "source": [
    "**CHECKPOINT: IF UNHAPPY WITH STEP1 SUMMARY RESULTS GO BACK AND MAKE A DIFF READ THRESHOLD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509024b3-fba0-451d-8752-ed8876724533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add gene information to step1 map of barcodes associted to a specific dna sequence \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Summary table\n",
    "Map4_Summary_Dict = {\n",
    "    'Category': [],\n",
    "    'Read Count': []\n",
    "}\n",
    "\n",
    "# Read reference LUT\n",
    "excel_df = pd.read_csv(gene_lut)\n",
    "\n",
    "###### adding new seq to gene lut end #UPDATE comment out from here until specified end if you don't want to add a new sequence to your gene lut \n",
    "# --- Add new sequence to reference LUT --- #UPDATE I frequently forget to add the GS linker control to my gene look up table so this ensure its added. Update if you use a diff one or want to add aditional seqs to your gene look up table before mapping to step 1\n",
    "new_seq = \"GGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGCGGCAGC\"\n",
    "\n",
    "if not (excel_df['DNA Sequence'].str.strip() == new_seq).any(): #U{DATE other info associated with you new gene varient \n",
    "    new_row = {\n",
    "        'DNA Sequence': new_seq,\n",
    "        'Gene Name': 'GS_only_seq',\n",
    "        'Mutant Sequence': 'GSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGSGS',\n",
    "        'Mutation': 'Gs_Only'\n",
    "    }\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    print(f\"Added new sequence to {gene_lut}\")\n",
    "else:\n",
    "    print(f\"Sequence already present in {gene_lut}\")\n",
    "\n",
    "# (Optional) Save updated LUT permanently\n",
    "excel_df.to_csv(gene_lut, index=False)\n",
    "\n",
    "###### adding new seq to gene lut end\n",
    "\n",
    "# Read the experimental CSV\n",
    "new_csv_df = pd.read_csv(step1_initial)\n",
    "\n",
    "# --- CUT-OFF SEQ CLEANING FUNCTION ---\n",
    "def clean_tiles_column(tiles_column):\n",
    "    cut_off_seq = 'GGATCCGAGCTCGCTAGC' #UPDATE to remove part of sequence from all the sequences in step1, it currently removes this sequence and eveyrthing after it  \n",
    "    cleaned = []\n",
    "    for seq in tiles_column:\n",
    "        seq = str(seq).strip()\n",
    "        cut_index = seq.find(cut_off_seq)\n",
    "        if cut_index != -1:\n",
    "            seq = seq[:cut_index]\n",
    "        cleaned.append(seq)\n",
    "    return cleaned\n",
    "\n",
    "# Apply cut-off sequence cleaning to Tiles column\n",
    "new_csv_df['Designed'] = clean_tiles_column(new_csv_df['Designed'])\n",
    "\n",
    "# Make a copy of the cleaned dataframe\n",
    "new_df = new_csv_df.copy()\n",
    "\n",
    "# Initialize new columns #UPDATE with additional columns you want or change to the way you named them \n",
    "new_df['Gene Name'] = None\n",
    "new_df['Mutant Sequence'] = None\n",
    "new_df['Mutation'] = None\n",
    "\n",
    "\n",
    "# Match cleaned Tiles against DNA Sequence in reference #UPDATE column names if nessicarry \n",
    "for i, tile in new_df.iterrows():\n",
    "    match = excel_df[excel_df['DNA Sequence'].str.strip() == tile['Designed']]\n",
    "    if not match.empty:\n",
    "        new_df.at[i, 'Gene Name'] = match['Gene Name'].values[0]\n",
    "        new_df.at[i, 'Mutant Sequence'] = match['Mutant Sequence'].values[0]\n",
    "        new_df.at[i, 'Mutation'] = match['Mutation'].values[0]\n",
    "\n",
    "# Print initial shape\n",
    "print(\"Step1 Map Initial Shape:\", new_df.shape)\n",
    "Map4_Summary_Dict['Category'].append(\"Step1 Map Initial Shape:\")\n",
    "Map4_Summary_Dict['Read Count'].append(new_df.shape[0])\n",
    "\n",
    "# Identify rows with no matches\n",
    "no_match_df = new_df[\n",
    "    new_df['Gene Name'].isna() &\n",
    "    new_df['Mutant Sequence'].isna()\n",
    "]\n",
    "\n",
    "# Drop unmatched rows\n",
    "new_df.dropna(subset=['Gene Name', 'Mutant Sequence'], inplace=True)\n",
    "\n",
    "# Print post-match shape\n",
    "print(\"Step1 Map Shape after dropping rows with no matches:\", new_df.shape)\n",
    "Map4_Summary_Dict['Category'].append(\"Step1 Map Shape after dropping rows with no matches:\")\n",
    "Map4_Summary_Dict['Read Count'].append(new_df.shape[0])\n",
    "\n",
    "# Log unmatched rows\n",
    "Map4_Summary_Dict['Category'].append(\"Number of Rows with No Tile Match:\")\n",
    "Map4_Summary_Dict['Read Count'].append(no_match_df.shape[0])\n",
    "\n",
    "print('No matches:')\n",
    "display(no_match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955dc76-f16c-4528-82e9-3f225ee5d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the new step1 map with gene info associated with barcode info \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21325b63-c46f-42e4-8344-ad144b634081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create files and summary table of step1 decoding \n",
    "new_df.to_excel(os.path.join(out_s1, f'{Lib_Name}_Step1_Decoded.xlsx'), index= False)\n",
    "new_df.to_csv(os.path.join(out_s1, f'{Lib_Name}_Step1_Decoded.csv'), index=False)\n",
    "\n",
    "Summary_Dict_df = pd.DataFrame.from_dict(Map4_Summary_Dict)\n",
    "Summary_Dict_df.to_csv(os.path.join(out_s1, f'{Lib_Name}_Step1_Decoded_Summary.csv'), index= False)\n",
    "Summary_Dict_df.to_excel(os.path.join(out_s1, f'{Lib_Name}_Step1_Decoded_Summary.xlsx'), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f39b4-f15d-4588-8af5-2448fe1afafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the number of genes in the gene look up table \n",
    "total_tiles_per_gene = excel_df['Gene Name'].value_counts()\n",
    "display(total_tiles_per_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1411e6f-152b-498b-ba92-90decf1c8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each Gene occurs\n",
    "gene_counts = new_df['Gene Name'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of Unique Barcode Combinations for each Gene:\")\n",
    "display(gene_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e00484-4b24-4055-a3e4-de917623c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gene counts from the new dataframe normalized to the potenital number of tiles for that gene \n",
    "gene_counts = new_df['Gene Name'].value_counts()\n",
    "\n",
    "# Calculate total tiles per gene from the lookup table\n",
    "total_tiles_per_gene = excel_df['Gene Name'].value_counts()\n",
    "\n",
    "# Create a new dataframe to ensure we match by gene name\n",
    "count_df = pd.DataFrame({'Gene Name': gene_counts.index, 'Gene Count': gene_counts.values})\n",
    "total_tiles_df = pd.DataFrame({'Gene Name': total_tiles_per_gene.index, 'Total Possible Tiles': total_tiles_per_gene.values})\n",
    "\n",
    "# Merge the two dataframes on the Gene column\n",
    "merged_df = pd.merge(count_df, total_tiles_df, on='Gene Name')\n",
    "\n",
    "# Normalize the gene counts\n",
    "merged_df['Normalized Count'] = (merged_df['Gene Count'] / merged_df['Total Possible Tiles']).round(1)\n",
    "\n",
    "#Order by largest to smallest normalized count \n",
    "merged_df = merged_df.sort_values(by='Normalized Count', ascending=False)\n",
    "display(merged_df)\n",
    "\n",
    "# Plot the normalized counts as a histogram\n",
    "plt.figure(figsize=(16, 6))\n",
    "merged_df.set_index('Gene Name')['Normalized Count'].plot(kind='bar')\n",
    "plt.xlabel('Gene')\n",
    "plt.ylabel('Normalized Count')\n",
    "plt.title('Histogram of Normalized Gene Counts')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'{Lib_Name}_decoding_S1/{Lib_Name}_Normalized_Gene_counts.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4c171-50ea-40c8-9a27-50fe1f6d401e",
   "metadata": {},
   "source": [
    "## 2. Adds Decoded Step1 Info to RNA Data and Calculates Activity Ratios\n",
    "- Adds the recently added gene, mutation, protein sequence information from decoded step1 to the RNA data based on matching unique ADBC + RTBC combinations\n",
    "- Activity ratios are calculated by RTBC UMI counts / ADBC UMI counts. A simple and complex ratio are created based on the way the UMIS were counted. The ratios highly correlate based on initial data, so I typically just use complex ratio for further downstream analysis and graphing\n",
    "- Controls only and Varibles only files are created if you want to look at the data with the controls pulled out or compare the datasets seperately \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f33cc-7505-4809-9454-33af626f841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rna = 'results_with_step1_unerror_corrected_with_zeros.csv' #UPDATE pathway to RNA data with initial UMI processing that maps them back to step1 genes via Sanjana pipeline \n",
    "\n",
    "umimin = 20 #UPDATE to change minimum UMI threshold\n",
    "\n",
    "output_dir = f'{Lib_Name}_output_ratio_files' # Define the output directory for this section\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42102ce-a175-4743-898d-ec48fb73242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_rna)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae080904-ef46-4285-a239-1acf91542df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = pd.read_csv('') #if you are skipping step1 beacuse you already have decoded step1 add it here \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b7bc1-859f-4412-bbf4-c284184611f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Initial length prints ----\n",
    "print(\"Initial length of RNA Data (df):\", len(df))\n",
    "print(\"Initial length of Step1 Map Decoded (new_df):\", len(new_df))\n",
    "\n",
    "# Clean: ensure Cat column exists and is normalized\n",
    "df['Cat'] = df['Cat'].astype(str).str.strip()\n",
    "new_df['Cat'] = new_df['Cat'].astype(str).str.strip()\n",
    "\n",
    "# Ensure new_df has only ONE row per Cat (prevents duplicate merge blow-ups)\n",
    "new_df = new_df.groupby('Cat', as_index=False).first()\n",
    "\n",
    "# Select only the columns we want to add\n",
    "cols_to_add = ['Cat', 'Mutant Sequence', 'Gene Name', 'Mutation']\n",
    "missing_cols = [c for c in cols_to_add if c not in new_df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"new_df is missing columns: {missing_cols}\")\n",
    "\n",
    "# Perform safe left merge (keeps df length the same as the original df)\n",
    "df = df.merge(\n",
    "    new_df[cols_to_add],\n",
    "    on='Cat',\n",
    "    how='left',\n",
    "    validate='many_to_one'   # df = many rows per Cat, new_df = one row per Cat\n",
    ")\n",
    "\n",
    "# Save results\n",
    "df.to_excel(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_mapped_to_S1_with_NANs.xlsx'), index=False)\n",
    "df.to_csv(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_mapped_to_S1_with_NANs.csv'), index=False)\n",
    "\n",
    "# OPTIONAL: drop rows where ALL 3 new columns are untouched NaN\n",
    "df_filtered = df.dropna(subset=['Mutant Sequence', 'Gene Name', 'Mutation'], how='all')\n",
    "\n",
    "print(\"Length of df after merge:\", len(df))\n",
    "print(\"Length of df after removing unmatched rows:\", len(df_filtered))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0721372-768c-4bde-b33b-af15361ebeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ratio_Simple and Ratio_Complex columns\n",
    "df['Ratio_Simple'] = df['RTBC_umi_count_simple'] / df['AD_umi_count_simple']\n",
    "df['Ratio_Complex'] = df['RTBC_umi_count_complex'] / df['AD_umi_count_complex']\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "df[['RTBC_umi_count_simple', 'AD_umi_count_simple', 'Ratio_Simple',\n",
    "    'RTBC_umi_count_complex', 'AD_umi_count_complex', 'Ratio_Complex']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52893303-cebd-42f8-8c3e-2050a96f03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_excel(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_with_ratios.xlsx'), index=False)\n",
    "df.to_csv(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_with_ratios.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0b0ca-9356-4d49-9fa4-bab4a73dc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a controls only df (optional)\n",
    "controls_df = df[df['Gene Name'] != 'NKX2_2_AD'] #UPDATE to include all gene names if using a library with multiple genes \n",
    "print(\"controls_df length:\", len(controls_df))\n",
    "\n",
    "# Number of unique entries in Designed column\n",
    "unique_designed = controls_df['Designed'].nunique()\n",
    "print(\"controls_df unique Tiles values:\", unique_designed)\n",
    "\n",
    "\n",
    "#save files \n",
    "controls_df.to_excel(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_with_ratios_Controls_Only.xlsx'), index=False)\n",
    "controls_df.to_csv(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_with_ratios_Controls_Only.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2969975-edd0-4daa-a8f4-9cfd4b4159d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a varients only no controls df (optional)\n",
    "No_Controls_df = df[df['Gene Name'] == 'NKX2_2_AD'] #UPDATE to include all gene names if using a library with multiple genes\n",
    "print(\"No_Controls_df length:\", len(No_Controls_df))\n",
    "\n",
    "# Number of unique entries in Designed column\n",
    "unique_designed_nc = No_Controls_df['Designed'].nunique()\n",
    "print(\"No_Controls_df unique Tiles values:\", unique_designed_nc)\n",
    "\n",
    "#save files \n",
    "No_Controls_df.to_excel(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_with_ratios_Variables_Only.xlsx'), index=False)\n",
    "No_Controls_df.to_csv(os.path.join(output_dir, f'{Lib_Name}_UMI_RNA_results_with_ratios_Varibles_Only.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b225a-fb9e-4e8a-9a67-d05582516075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames based on unique values in the 'number' column\n",
    "# Sort the unique numbers first so they are in the right order in the dictionary \n",
    "unique_numbers = sorted(df['number'].unique())\n",
    "\n",
    "# Build dictionary \n",
    "dataframes = {f\"df_{num}\": df[df['number'] == num] for num in unique_numbers}\n",
    "\n",
    "# Print length and unique count in 'Designed' column for each subset\n",
    "for key, sub_df in dataframes.items():\n",
    "    length = len(sub_df)\n",
    "    unique_designed = sub_df['Designed'].nunique()\n",
    "    print(f\"{key}: length={length}, unique Designed={unique_designed}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c42821-60da-42a8-bd08-bf57d5ffb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340731d0-f8ef-46dc-ac66-0af1d4d5f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to dfs to have a minimum number of UMIs for the ADBC and RTBC\n",
    "filtered_dataframes = {\n",
    "    name: df[(df['AD_umi_count_complex'] >= f'{umimin}') & (df['RTBC_umi_count_complex'] >= f'{umimin}')]\n",
    "    for name, df in dataframes.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9aafbf-2083-4e02-84b1-27ae21611004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_dataframes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45bf94-2e6e-4eff-aa0d-b9d1bd0ba905",
   "metadata": {},
   "source": [
    "## 3. Make Lots of Graphs \n",
    "- this section relies on the number column created during Sanjana's initial RNA processing that is done before this file to seperate the large df into smaller dfs based on differing samples\n",
    "- it is critical to ensure you have the right numbers associated with the right sample type\n",
    "- all analysis will be done using the complex ratio and the df that contains all of the variables and controls together filtered >=20UMI, change if needed but if you have UMI counts of zeros kept in you will need to at least remove those before graphing or it will cause errors\n",
    "- histograms, bar graphs, and correlations will be made for the correlations and histograms that involve paired dfs then you need to specify in the tuple which dfs you want to analyse together and it will loop through all combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4cbc2-a6be-4569-b283-0dde26a8a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for this section\n",
    "os.makedirs(f'{Lib_Name}_output_graphs', exist_ok=True)\n",
    "names = ['R1_Notind_24hr_S1', #UPDATE files are called by number if you want to associate a more meaninful name with the graphs/file names list them in number order here \n",
    "         'R1_Ind_24hr_S2',\n",
    "         'R2_Notind_24hr_S3', \n",
    "         'R2_Ind_24hr_S4',\n",
    "         'R1_Notind_48hr_S5', \n",
    "         'R1_Ind_48hr_S6',\n",
    "         'R2_Notind_48hr_S7', \n",
    "         'R2_Ind_48hr_S8']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e632ac0-b871-4fb7-90a8-91afcf2166c8",
   "metadata": {},
   "source": [
    "**HISTOGRAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e3a64-fc87-4a55-8135-7a935d71930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complex Ratio Individual Samples Histograms (normal, log10, xscale log)\n",
    "# log10 seems to give the best visibility but it creates versions of all of them \n",
    "\n",
    "os.makedirs(f'{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio', exist_ok=True)\n",
    "\n",
    "# Loop through each unique number and plot histograms using filtered_dataframes\n",
    "for i, num in enumerate(sorted(df['number'].unique())):\n",
    "    key = f\"df_{num}\"                     # dictionary key\n",
    "    df_current = filtered_dataframes[key] # grab the subset DataFrame\n",
    "    title = f\"{num} - {names[i]}\"\n",
    "    base_filename = f\"{num}_{names[i].replace(' ', '_')}\"\n",
    "\n",
    "    # --- 1. Regular histogram ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(df_current['Ratio_Complex'].dropna(), bins=30,\n",
    "             color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"{title} - Ratio_Complex\")\n",
    "    plt.xlabel('Ratio_Complex')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio/{base_filename}_Ratio_Complex_histogram.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 2. Histogram with log-scaled x-axis ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(df_current['Ratio_Complex'].dropna(), bins=30,\n",
    "             color='orange', edgecolor='black')\n",
    "    plt.xscale('log')  # log scale on x-axis\n",
    "    plt.title(f\"{title} - Ratio_Complex (X-axis Log Scale)\")\n",
    "    plt.xlabel('Ratio_Complex (log scale)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio/{base_filename}_Ratio_Complex_xlog_histogram.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 3. Histogram of log10-transformed values ---\n",
    "    ratio_log10 = np.log10(df_current['Ratio_Complex'].dropna())\n",
    "    ratio_log10 = ratio_log10.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(ratio_log10, bins=30, color='salmon', edgecolor='black')\n",
    "    plt.title(f\"{title} - Log10(Ratio_Complex)\")\n",
    "    plt.xlabel('Log10(Ratio_Complex)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio/{base_filename}_Ratio_Complex_log10_histogram.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd7984-88c0-4568-b315-8079f9fbd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#induced complex ratio / uninduced complex ratio (yes ratio of a ratio)\n",
    "# Create output directory\n",
    "output_dir4 = f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio_induced_divided_by_uninduced\"\n",
    "os.makedirs(output_dir4, exist_ok=True)\n",
    "\n",
    "# Define merge key \n",
    "merge_key = \"Cat\"   #UPDATE if you want to merge on a different column  \n",
    "\n",
    "# Define tuple combinations (first ÷ second)\n",
    "tuple_combinations = [('df_4', 'df_3'), ('df_8', 'df_7')]  #UPDATE (first df is induced, second is UNinduced) (first_df/second_df)\n",
    "\n",
    "# Loop through each tuple combination\n",
    "for induced_key, uninduced_key in tuple_combinations:\n",
    "    df_induced = filtered_dataframes[induced_key][[merge_key, 'Ratio_Complex']].dropna()\n",
    "    df_uninduced = filtered_dataframes[uninduced_key][[merge_key, 'Ratio_Complex']].dropna()\n",
    "\n",
    "    # Merge on chosen key\n",
    "    merged = df_induced.merge(df_uninduced, on=merge_key, suffixes=('_induced', '_uninduced'))\n",
    "\n",
    "    # Compute ratio (induced ÷ uninduced)\n",
    "    merged['Ratio_Complex_divided'] = merged['Ratio_Complex_induced'] / merged['Ratio_Complex_uninduced']\n",
    "    ratio_data = merged['Ratio_Complex_divided'].dropna()\n",
    "\n",
    "    # --- 1. Normal histogram ---\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(ratio_data, bins=50, color='mediumseagreen', edgecolor='black')\n",
    "    plt.title(f'{induced_key} ÷ {uninduced_key} Ratio_Complex by {merge_key} (Normal)')\n",
    "    plt.xlabel(f'{induced_key} ÷ {uninduced_key} Ratio_Complex')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir4, f'{induced_key}_div_{uninduced_key}_ratio_hist_normal.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- 2. Log10-transformed histogram ---\n",
    "    ratio_log10 = np.log10(ratio_data.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(ratio_log10, bins=50, color='mediumseagreen', edgecolor='black')\n",
    "    plt.title(f'{induced_key} ÷ {uninduced_key} Ratio_Complex by {merge_key} (Log10 transformed)')\n",
    "    plt.xlabel(f'Log10({induced_key} ÷ {uninduced_key} Ratio_Complex)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir4, f'{induced_key}_div_{uninduced_key}_ratio_hist_log10.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- 3. Histogram with log-scaled x-axis ---\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(ratio_data, bins=50, color='mediumseagreen', edgecolor='black')\n",
    "    plt.xscale('log')\n",
    "    plt.title(f'{induced_key} ÷ {uninduced_key} Ratio_Complex by {merge_key} (X-axis Log Scale)')\n",
    "    plt.xlabel(f'{induced_key} ÷ {uninduced_key} Ratio_Complex (log scale)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir4, f'{induced_key}_div_{uninduced_key}_ratio_hist_xlog.png'), dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df645a0-123c-4c5a-b950-09a5a903bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph UMI per unique Integration Histograms (normal, log10, xscale log)\n",
    "#the graph should already be in a place where each row is unique bc combination / integration so the code works based on that assumption \n",
    "#this one log scale x is usually best visibility but overall usually hard to get good resolution with the spread\n",
    "\n",
    "output_dir2 = f\"{Lib_Name}_output_graphs/{Lib_Name}_umi_per_integration\"\n",
    "os.makedirs(output_dir2, exist_ok=True)\n",
    "\n",
    "# Loop through each dataframe in the filtered_dataframes dictionary\n",
    "for name, df in filtered_dataframes.items():\n",
    "\n",
    "    # --- AD_umi_count_complex ---\n",
    "    if 'AD_umi_count_complex' in df.columns:\n",
    "        values = df['AD_umi_count_complex'].dropna()\n",
    "\n",
    "        # Stats\n",
    "        total_umi = values.sum()\n",
    "        unique_cat = df['Cat'].nunique() if 'Cat' in df.columns else 0\n",
    "        stats_text = f\"Total UMI: {total_umi}\\nUnique Cat: {unique_cat}\"\n",
    "\n",
    "        # 1. Normal histogram\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values, bins=500, color='steelblue', alpha=0.6)\n",
    "        plt.title(f'{name} - ADBC (Normal)')\n",
    "        plt.xlabel('AD_umi_count_complex')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, va='top', ha='right',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir2, f'{name}_ADBC_hist.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. Log10-transformed histogram\n",
    "        values_log10 = np.log10(values.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values_log10, bins=500, color='steelblue', alpha=0.6)\n",
    "        plt.title(f'{name} - ADBC (Log10 transformed)')\n",
    "        plt.xlabel('Log10(AD_umi_count_complex)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, va='top', ha='right',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir2, f'{name}_ADBC_log10_hist.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Histogram with log-scaled x-axis\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values, bins=500, color='steelblue', alpha=0.6)\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'{name} - ADBC (X-axis Log Scale)')\n",
    "        plt.xlabel('AD_umi_count_complex (log scale)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, va='top', ha='right',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir2, f'{name}_ADBC_xlog_hist.png'))\n",
    "        plt.close()\n",
    "\n",
    "    # --- RTBC_umi_count_complex ---\n",
    "    if 'RTBC_umi_count_complex' in df.columns:\n",
    "        values = df['RTBC_umi_count_complex'].dropna()\n",
    "\n",
    "        # Stats\n",
    "        total_umi = values.sum()\n",
    "        unique_cat = df['Cat'].nunique() if 'Cat' in df.columns else 0\n",
    "        stats_text = f\"Total UMI: {total_umi}\\nUnique Cat: {unique_cat}\"\n",
    "\n",
    "        # 1. Normal histogram\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values, bins=500, color='darkorange', alpha=0.6)\n",
    "        plt.title(f'{name} - RTBC (Normal)')\n",
    "        plt.xlabel('RTBC_umi_count_complex')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, va='top', ha='right',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir2, f'{name}_RTBC_hist.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. Log10-transformed histogram\n",
    "        values_log10 = np.log10(values.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values_log10, bins=500, color='darkorange', alpha=0.6)\n",
    "        plt.title(f'{name} - RTBC (Log10 transformed)')\n",
    "        plt.xlabel('Log10(RTBC_umi_count_complex)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, va='top', ha='right',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir2, f'{name}_RTBC_log10_hist.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Histogram with log-scaled x-axis\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(values, bins=500, color='darkorange', alpha=0.6)\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'{name} - RTBC (X-axis Log Scale)')\n",
    "        plt.xlabel('RTBC_umi_count_complex (log scale)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes,\n",
    "                 fontsize=9, va='top', ha='right',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir2, f'{name}_RTBC_xlog_hist.png'))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036706d-be00-45f4-a676-0afb53c00f64",
   "metadata": {},
   "source": [
    "**CORRELATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584be99-cda0-43fc-88ba-932ad6888934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complex Ratio Correlations \n",
    "#list the tuple combinations you want to make correlations with  \n",
    "\n",
    "# Step 1: Create output directory\n",
    "output_dir3 = f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio_correlations\"\n",
    "os.makedirs(output_dir3, exist_ok=True)\n",
    "\n",
    "# Step 2: Define merge key \n",
    "merge_key = \"Cat\"   #UPDATE with a diff column if you want \n",
    "\n",
    "# Step 3: Define tuple combinations of DataFrame keys to compare\n",
    "tuple_combinations = [('df_1', 'df_2'),('df_3', 'df_4'),('df_5', 'df_6'), ('df_7', 'df_8')] #induced vs uninduced \n",
    "#[('df_1', 'df_3'),('df_2', 'df_4'),('df_5', 'df_7'), ('df_6', 'df_8')] #R1 vs R2\n",
    "#[('df_1', 'df_5'),('df_2', 'df_6'),('df_3', 'df_7'), ('df_4', 'df_8')] #24 vs 48\n",
    "\n",
    "# Step 4: Loop through each tuple combination\n",
    "for key_a, key_b in tuple_combinations:\n",
    "    df_a = filtered_dataframes[key_a][[merge_key, 'Ratio_Complex']].dropna()\n",
    "    df_b = filtered_dataframes[key_b][[merge_key, 'Ratio_Complex']].dropna()\n",
    "\n",
    "    # Merge on chosen key\n",
    "    merged = df_a.merge(df_b, on=merge_key, suffixes=(f'_{key_a}', f'_{key_b}'))\n",
    "\n",
    "    # Compute correlation\n",
    "    x = merged[f'Ratio_Complex_{key_a}']\n",
    "    y = merged[f'Ratio_Complex_{key_b}']\n",
    "    r, _ = pearsonr(x, y)\n",
    "    r_squared = r ** 2\n",
    "\n",
    "    # Count matches and unmatched\n",
    "    matched_count = len(merged)\n",
    "    unmatched_a = len(df_a) - matched_count\n",
    "    unmatched_b = len(df_b) - matched_count\n",
    "\n",
    "    # Plot correlation\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x, y, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    plt.xlabel(f'{key_a} Ratio_Complex ({merge_key})')\n",
    "    plt.ylabel(f'{key_b} Ratio_Complex ({merge_key})')\n",
    "    plt.title(f'Correlation of Ratio_Complex by {merge_key}: {key_a} vs {key_b}')\n",
    "\n",
    "    # Annotate with stats in top right\n",
    "    textstr = (\n",
    "        f'Matched {merge_key}s: {matched_count}\\n'\n",
    "        f'Unmatched in {key_a}: {unmatched_a}\\n'\n",
    "        f'Unmatched in {key_b}: {unmatched_b}\\n'\n",
    "        f'r = {r:.3f}\\n'\n",
    "        f'r² = {r_squared:.3f}'\n",
    "    )\n",
    "    plt.text(0.95, 0.95, textstr, transform=plt.gca().transAxes,\n",
    "             fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot with merge_key in filename\n",
    "    output_path = os.path.join(\n",
    "        output_dir3,\n",
    "        f'{key_a}_vs_{key_b}_correlation_by_{merge_key}.png'\n",
    "    )\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061bc11-8c48-4bce-ad54-2facfba3bf98",
   "metadata": {},
   "source": [
    "**OUTLIER REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5387aa-a2c2-4634-a503-f4693e7ea3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Integration outliers within each tile based on z-score\n",
    "\n",
    "output_dir5 = f\"{Lib_Name}_output_graphs/{Lib_Name}_averge_ratio_complex_per_tile_outliers_removed\"\n",
    "os.makedirs(output_dir5, exist_ok=True)\n",
    "\n",
    "average_ratio_complex_dfs = {}\n",
    "\n",
    "for name, df in filtered_dataframes.items():\n",
    "    df = df.copy()\n",
    "    df['Mutation'] = df['Mutation'].replace('1-2-3-4', 'WT') #THIS is just beacuse the shuffle in 1-2-3-4 order is just WT and I forgot to update earlier\n",
    "\n",
    "    grouped = df.groupby('Mutation')\n",
    "    records = []\n",
    "\n",
    "    for mutation, group in grouped:\n",
    "        raw_ratios = group['Ratio_Complex'].dropna()\n",
    "        initial_count = len(raw_ratios)\n",
    "\n",
    "        # --- Z-score filtering ---\n",
    "        if len(raw_ratios) > 1:\n",
    "            z_scores = zscore(raw_ratios)\n",
    "            cleaned_ratios = raw_ratios[(np.abs(z_scores) <= 3)]\n",
    "        else:\n",
    "            # keep single-value groups\n",
    "            cleaned_ratios = raw_ratios.copy()\n",
    "\n",
    "        avg = cleaned_ratios.mean()\n",
    "        stderr = sem(cleaned_ratios) if len(cleaned_ratios) > 1 else np.nan\n",
    "        count_cleaned = len(cleaned_ratios)\n",
    "\n",
    "        # Print counts before and after\n",
    "        #print(f\"[{name}] Mutation {mutation}: before={initial_count}, after={count_cleaned}\")\n",
    "\n",
    "        # Safe extraction of metadata\n",
    "        mutant_seq = group['Mutant Sequence'].iloc[0] if 'Mutant Sequence' in group.columns else ''\n",
    "        gene_name = group['Gene Name'].iloc[0] if 'Gene Name' in group.columns else ''\n",
    "\n",
    "        records.append({\n",
    "            'Mutant Sequence': mutant_seq,\n",
    "            'Gene Name': gene_name,\n",
    "            'Mutation': mutation,\n",
    "            'average_Ratio_Complex': avg,\n",
    "            'stderror_Ratio_Complex': stderr,\n",
    "            'number_integrations': count_cleaned,\n",
    "            'initial_ints': initial_count\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    summary_df = summary_df.sort_values(by='average_Ratio_Complex', ascending=False)\n",
    "\n",
    "    # # Optional: global z-score of averages\n",
    "    # if summary_df['average_Ratio_Complex'].std() == 0:\n",
    "    #     summary_df['zscore_average_Ratio_Complex'] = np.nan\n",
    "    # else:\n",
    "    #     summary_df['zscore_average_Ratio_Complex'] = (\n",
    "    #         (summary_df['average_Ratio_Complex'] - summary_df['average_Ratio_Complex'].mean()) /\n",
    "    #         summary_df['average_Ratio_Complex'].std()\n",
    "    #     )\n",
    "\n",
    "    key = f'average_ratio_complex_outliers_removed_{name}'\n",
    "    average_ratio_complex_dfs[key] = summary_df\n",
    "\n",
    "    summary_df.to_csv(os.path.join(output_dir5, f'{key}.csv'), index=False)\n",
    "    summary_df.to_excel(os.path.join(output_dir5, f'{key}.xlsx'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbbbf8-077f-4f5c-80e8-b9c647c1d82d",
   "metadata": {},
   "source": [
    "**CORRELATIONS WITH OUTLIERS REMOVED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94105a7f-715e-4cc6-bd36-7d78496836fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Complex Ratio (with outliers removed) per Tile Correlations \n",
    "\n",
    "output_dir6 = f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio_correlation_nooutliers\"\n",
    "os.makedirs(output_dir6, exist_ok=True)\n",
    "\n",
    "# Step 1: Folder path containing CSVs\n",
    "folder_path = output_dir5  # ← Replace with your actual folder path\n",
    "nooutliers = {}\n",
    "\n",
    "# Load all CSVs into dictionary\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        nooutliers[key] = df\n",
    "\n",
    "# Step 2: Define tuple combinations of DataFrame keys\n",
    "tuple_combinations = [\n",
    "    #ind vs unind combinations \n",
    "    ('average_ratio_complex_outliers_removed_df_1', 'average_ratio_complex_outliers_removed_df_2'),\n",
    "    ('average_ratio_complex_outliers_removed_df_3', 'average_ratio_complex_outliers_removed_df_4'),\n",
    "    ('average_ratio_complex_outliers_removed_df_5', 'average_ratio_complex_outliers_removed_df_6'),\n",
    "    ('average_ratio_complex_outliers_removed_df_7', 'average_ratio_complex_outliers_removed_df_8')\n",
    "   \n",
    "    ##R1 vs R2 combinations\n",
    "    #, ('average_ratio_complex_outliers_removed_df_1', 'average_ratio_complex_outliers_removed_df_3'),\n",
    "    #('average_ratio_complex_outliers_removed_df_2', 'average_ratio_complex_outliers_removed_df_4'),\n",
    "    #('average_ratio_complex_outliers_removed_df_5', 'average_ratio_complex_outliers_removed_df_7'),\n",
    "   # ('average_ratio_complex_outliers_removed_df_6', 'average_ratio_complex_outliers_removed_df_8')\n",
    "    \n",
    "    ##24 vs 48 combinations\n",
    "    #, ('average_ratio_complex_outliers_removed_df_1', 'average_ratio_complex_outliers_removed_df_5'),\n",
    "    #('average_ratio_complex_outliers_removed_df_2', 'average_ratio_complex_outliers_removed_df_6'),\n",
    "    #('average_ratio_complex_outliers_removed_df_3', 'average_ratio_complex_outliers_removed_df_7'),\n",
    "   # ('average_ratio_complex_outliers_removed_df_4', 'average_ratio_complex_outliers_removed_df_8')\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "# Helper to shorten labels (strip prefix, keep df_X)\n",
    "def short_label(key):\n",
    "    return key.split('_')[-1] if key.startswith('average_ratio_complex_outliers_removed') else key\n",
    "\n",
    "# Step 3: Loop through each tuple combination\n",
    "for induced_key, uninduced_key in tuple_combinations:\n",
    "    df_induced = nooutliers[induced_key][['Mutation', 'average_Ratio_Complex']].dropna()\n",
    "    df_uninduced = nooutliers[uninduced_key][['Mutation', 'average_Ratio_Complex']].dropna()\n",
    "\n",
    "    # Merge on Mutation\n",
    "    merged = df_induced.merge(df_uninduced, on='Mutation', suffixes=('_induced', '_uninduced'))\n",
    "\n",
    "    # Compute correlation\n",
    "    x = merged['average_Ratio_Complex_uninduced']\n",
    "    y = merged['average_Ratio_Complex_induced']\n",
    "    r, _ = pearsonr(x, y)\n",
    "    r_squared = r ** 2\n",
    "\n",
    "    # Count matches and unmatched\n",
    "    matched_count = len(merged)\n",
    "    unmatched_uninduced = len(df_uninduced) - matched_count\n",
    "    unmatched_induced = len(df_induced) - matched_count\n",
    "\n",
    "    # Short labels for plotting\n",
    "    induced_short = short_label(induced_key)\n",
    "    uninduced_short = short_label(uninduced_key)\n",
    "\n",
    "    # Plot correlation\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x, y, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "    plt.xlabel(f'{uninduced_short} average_Ratio_Complex No Outliers')\n",
    "    plt.ylabel(f'{induced_short} average_Ratio_Complex No Outliers')\n",
    "    plt.title(f'Correlation of average_Ratio_Complex No Outliers: {induced_short} vs {uninduced_short}')\n",
    "\n",
    "    # Annotate with stats in top left\n",
    "    textstr = (\n",
    "        f'Matched: {matched_count}\\n'\n",
    "        f'Unmatched in {uninduced_short}: {unmatched_uninduced}\\n'\n",
    "        f'Unmatched in {induced_short}: {unmatched_induced}\\n'\n",
    "        f'r = {r:.3f}\\n'\n",
    "        f'r² = {r_squared:.3f}'\n",
    "    )\n",
    "    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes,\n",
    "             fontsize=10, verticalalignment='top', horizontalalignment='left',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot with short names\n",
    "    output_path = os.path.join(output_dir6, f'{induced_short}_vs_{uninduced_short}_correlation_plot_NO_outliers.png')\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582714e2-3968-4f86-bbfb-f5c7e995525b",
   "metadata": {},
   "source": [
    "**GFP Activity for UMI >=20 UMI for RTBC and ADBC pair with outliers removed AND >= 5 Integrations per Tile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c554d6-62fe-4b73-a61b-8cb351099c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP Activity per tile number of integrations on top of bar color coded, same y-axis, graph breaks for better resolutoin on most common Ration range \n",
    "\n",
    "# GFP Activity Bar Graphs with broken y-axis, counts above bars, and summary legend\n",
    "input_folder = output_dir5\n",
    "int_min_per_tile = 5  #UPDATE minimum integrations per tile\n",
    "\n",
    "output_dir7 = f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio_nooutliers_{int_min_per_tile}_ints_activity\"\n",
    "os.makedirs(output_dir7, exist_ok=True)\n",
    "\n",
    "def short_df_name(filename):\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    return \"_\".join(base.split(\"_\")[-2:]) if \"df_\" in base else base\n",
    "\n",
    "# Collect all mutations for global color assignment\n",
    "all_mutations = []\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(input_folder, filename))\n",
    "        all_mutations.extend(df['Mutation'].dropna().unique().tolist())\n",
    "mutation_order = sorted(set(all_mutations))\n",
    "\n",
    "cmap = cm.get_cmap('tab20', len(mutation_order))\n",
    "mutation_colors = {mut: cmap(i) for i, mut in enumerate(mutation_order)}\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(input_folder, filename))\n",
    "        plot_df = df[df['number_integrations'] >= int_min_per_tile]\n",
    "        total_valid_rows = df[df['number_integrations'] >= 1].shape[0]\n",
    "        total_bars = plot_df.shape[0]\n",
    "\n",
    "        if plot_df.empty:\n",
    "            continue\n",
    "\n",
    "        df_short = short_df_name(filename)\n",
    "        present_mutations = [m for m in mutation_order if m in plot_df['Mutation'].values]\n",
    "        plot_df = plot_df.set_index('Mutation').loc[present_mutations]\n",
    "\n",
    "        # Broken y-axis with two panels\n",
    "        fig, (ax_main, ax_top) = plt.subplots(2, 1, sharex=True, figsize=(14, 8),\n",
    "                                              gridspec_kw={'height_ratios':[3,1]})\n",
    "\n",
    "        colors = [mutation_colors.get(mut, 'gray') for mut in plot_df.index]\n",
    "\n",
    "        # Plot bars on both axes\n",
    "        bars_main = ax_main.bar(plot_df.index, plot_df['average_Ratio_Complex'],\n",
    "                                yerr=plot_df['stderror_Ratio_Complex'], capsize=5,\n",
    "                                alpha=0.7, color=colors)\n",
    "        bars_top = ax_top.bar(plot_df.index, plot_df['average_Ratio_Complex'],\n",
    "                              yerr=plot_df['stderror_Ratio_Complex'], capsize=5,\n",
    "                              alpha=0.7, color=colors)\n",
    "\n",
    "        # Main axis shows 0–5\n",
    "        ax_main.set_ylim(0, 5)\n",
    "        # Top axis shows 5–16\n",
    "        ax_top.set_ylim(5, 16)\n",
    "\n",
    "        # Hide spines between subplots\n",
    "        ax_main.spines['top'].set_visible(False)\n",
    "        ax_top.spines['bottom'].set_visible(False)\n",
    "        ax_main.xaxis.tick_top()\n",
    "        ax_main.tick_params(labeltop=False)\n",
    "        ax_top.xaxis.tick_bottom()\n",
    "\n",
    "        # Add diagonal break marks only at the top of lower axis\n",
    "        d = .015\n",
    "        kwargs = dict(transform=ax_main.transAxes, color='k', clip_on=False)\n",
    "        ax_main.plot((-d,+d), (1-d,1+d), **kwargs)\n",
    "        ax_main.plot((1-d,1+d), (1-d,1+d), **kwargs)\n",
    "\n",
    "        # Labels and title\n",
    "        ax_main.set_ylabel('GFP Activity (Average Ratio Complex)')\n",
    "        fig.suptitle(f'{df_short} - Mutations with ≥{int_min_per_tile} Integrations')\n",
    "\n",
    "        plt.xticks(rotation=90, ha='center', fontsize=8)\n",
    "\n",
    "        # Add horizontal gridlines\n",
    "        ax_main.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax_top.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Add vertical dotted guide lines across the whole figure #UPDATE comment this out to remove vertical lines \n",
    "        ticks = ax_top.get_xticks()\n",
    "        for tick in ticks:\n",
    "            x_fig = ax_top.transData.transform((tick, 0))[0]\n",
    "            x_fig = fig.transFigure.inverted().transform((x_fig, 0))[0]\n",
    "            fig.add_artist(Line2D([x_fig, x_fig], [0, 1],\n",
    "                                  transform=fig.transFigure,\n",
    "                                  color='gray', linestyle=':', linewidth=0.5, alpha=0.5))\n",
    "\n",
    "        # Annotate counts above bars\n",
    "        for mut, avg, err, count in zip(plot_df.index,\n",
    "                                        plot_df['average_Ratio_Complex'],\n",
    "                                        plot_df['stderror_Ratio_Complex'],\n",
    "                                        plot_df['number_integrations']):\n",
    "            if pd.notnull(avg):\n",
    "                top_val = avg + err if pd.notnull(err) else avg\n",
    "                if top_val <= 5:\n",
    "                    ax = ax_main\n",
    "                else:\n",
    "                    ax = ax_top\n",
    "                ax.text(mut, top_val + 0.1, f'{int(count):,}',\n",
    "                        ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "        # Add legend with summary counts\n",
    "        legend_text = f'Total bars plotted ≥{int_min_per_tile} ints: {total_bars:,}\\nTotal rows with ≥1 ints: {total_valid_rows:,}'\n",
    "        ax_top.legend([legend_text], loc='upper right', frameon=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_filename = f'{df_short}_plot_min{int_min_per_tile}int_brokenY.png'\n",
    "        plt.savefig(os.path.join(output_dir7, plot_filename))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa8d83-abbb-47be-9f49-c7218a9ffdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GFP Activity Bar Graphs (leaves empty spaces in graph if it is missing that mutation)\n",
    "# input_folder = output_dir5\n",
    "# int_min_per_tile = 3  # UPDATE with the number of min integrations you want per tile\n",
    "\n",
    "# output_dir7 = f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio_nooutliers_{int_min_per_tile}_ints_activity\"\n",
    "# os.makedirs(output_dir7, exist_ok=True)\n",
    "\n",
    "# # Helper to shorten the filename to just df_X\n",
    "# def short_df_name(filename):\n",
    "#     base = os.path.splitext(filename)[0]\n",
    "#     return \"_\".join(base.split(\"_\")[-2:]) if \"df_\" in base else base\n",
    "\n",
    "# # --- Step 1: Determine global mutation order across all CSVs ---\n",
    "# all_mutations = []\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         filepath = os.path.join(input_folder, filename)\n",
    "#         df = pd.read_csv(filepath)\n",
    "#         all_mutations.extend(df['Mutation'].dropna().unique().tolist())\n",
    "\n",
    "# # Unique mutations in consistent order\n",
    "# mutation_order = sorted(set(all_mutations))\n",
    "\n",
    "# # Assign colors to mutations using a colormap\n",
    "# cmap = cm.get_cmap('tab20', len(mutation_order))  # tab20 has 20 distinct colors\n",
    "# mutation_colors = {mut: cmap(i) for i, mut in enumerate(mutation_order)}\n",
    "\n",
    "# # --- Step 2: Loop through all CSV files and plot ---\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         filepath = os.path.join(input_folder, filename)\n",
    "#         df = pd.read_csv(filepath)\n",
    "\n",
    "#         # Filter for plotting and total count\n",
    "#         plot_df = df[df['number_integrations'] >= int_min_per_tile]\n",
    "#         total_valid_rows = df[df['number_integrations'] >= 1].shape[0]\n",
    "#         total_bars = plot_df.shape[0]\n",
    "\n",
    "#         if plot_df.empty:\n",
    "#             continue  # skip if no valid rows\n",
    "\n",
    "#         # Short name for plot title and saved file\n",
    "#         df_short = short_df_name(filename)\n",
    "\n",
    "#         # Reindex plot_df to global mutation order (missing mutations will be NaN)\n",
    "#         plot_df = plot_df.set_index('Mutation').reindex(mutation_order)\n",
    "\n",
    "#         # Plot\n",
    "#         plt.figure(figsize=(14, 6))\n",
    "#         bars = plt.bar(\n",
    "#             plot_df.index,\n",
    "#             plot_df['average_Ratio_Complex'],\n",
    "#             yerr=plot_df['stderror_Ratio_Complex'],\n",
    "#             capsize=5,\n",
    "#             alpha=0.7,\n",
    "#             color=[mutation_colors.get(mut, 'gray') for mut in plot_df.index]\n",
    "#         )\n",
    "#         plt.xticks(rotation=90, ha='center', fontsize=8)\n",
    "#         plt.ylabel('GFP Activity (Average Ratio Complex)')\n",
    "#         plt.title(f'{df_short} - Mutations with ≥{int_min_per_tile} Integrations')\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#         # Annotate each bar above the error bar\n",
    "#         for bar, avg, err, count in zip(\n",
    "#             bars,\n",
    "#             plot_df['average_Ratio_Complex'],\n",
    "#             plot_df['stderror_Ratio_Complex'],\n",
    "#             plot_df['number_integrations']\n",
    "#         ):\n",
    "#             if pd.notnull(avg):\n",
    "#                 top = avg + err if pd.notnull(err) else avg\n",
    "#                 plt.text(bar.get_x() + bar.get_width() / 2, top + 0.01, f'{int(count):,}',\n",
    "#                          ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "#         # Add legend with summary counts\n",
    "#         legend_text = f'Total bars plotted ≥{int_min_per_tile} ints: {total_bars:,}\\nTotal rows with ≥1 ints: {total_valid_rows:,}'\n",
    "#         plt.legend([legend_text], loc='upper right', frameon=True)\n",
    "\n",
    "#         # Save figure to same folder with short name\n",
    "#         plot_filename = f'{df_short}_plot_min{int_min_per_tile}int.png'\n",
    "#         plot_path = os.path.join(output_dir7, plot_filename)\n",
    "#         plt.savefig(plot_path)\n",
    "#         plt.show()  # or plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3695e-1276-4890-8fb5-2f76f1544230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP Activity Induced complex ratio / Uninduced complex ratio for each mutation \n",
    "\n",
    "# Input folder\n",
    "input_folder = output_dir5\n",
    "int_min = 3\n",
    "\n",
    "#out_dir8 = f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_complex_ratio_nooutliers_{int_min}_ints_activity_ind_divide_by_unind\"\n",
    "base_graph_dir = f\"{Lib_Name}_output_graphs\"\n",
    "out_dir8 = os.path.join(base_graph_dir,\n",
    "                        f\"{Lib_Name}_output_graphs_complex_ratio_nooutliers_{int_min}_ints_activity_IND_vs_NI\")\n",
    "\n",
    "os.makedirs(out_dir8, exist_ok=True)\n",
    "print(\"Saving plots to:\", os.path.abspath(out_dir8))\n",
    "\n",
    "\n",
    "# List of file name pairs to compare (no .csv needed) \n",
    "#List the induced first and the unindueced second ****\n",
    "file_pairs = [\n",
    "    ('average_ratio_complex_outliers_removed_df_1', 'average_ratio_complex_outliers_removed_df_2'),\n",
    "    ('average_ratio_complex_outliers_removed_df_3', 'average_ratio_complex_outliers_removed_df_4'),\n",
    "    ('average_ratio_complex_outliers_removed_df_5', 'average_ratio_complex_outliers_removed_df_6'),\n",
    "    ('average_ratio_complex_outliers_removed_df_7', 'average_ratio_complex_outliers_removed_df_8')\n",
    "]\n",
    "\n",
    "# Get sorted CSV files\n",
    "csv_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.csv')])\n",
    "\n",
    "# --- Step 1: Determine global mutation order across all CSVs ---\n",
    "all_mutations = []\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(os.path.join(input_folder, filename))\n",
    "    all_mutations.extend(df['Mutation'].dropna().unique().tolist())\n",
    "mutation_order = sorted(set(all_mutations))\n",
    "\n",
    "# Assign colors to mutations using a colormap\n",
    "cmap = cm.get_cmap('tab20', len(mutation_order))\n",
    "mutation_colors = {mut: cmap(i) for i, mut in enumerate(mutation_order)}\n",
    "\n",
    "# --- Step 2: Loop through file pairs ---\n",
    "for file1_base, file2_base in file_pairs:\n",
    "    file1_name = file1_base + '.csv'\n",
    "    file2_name = file2_base + '.csv'\n",
    "\n",
    "    if file1_name not in csv_files or file2_name not in csv_files:\n",
    "        raise ValueError(f\"One of the specified files ({file1_name}, {file2_name}) is not in the folder.\")\n",
    "\n",
    "    file1_path = os.path.join(input_folder, file1_name)\n",
    "    file2_path = os.path.join(input_folder, file2_name)\n",
    "\n",
    "    # Load data\n",
    "    df1 = pd.read_csv(file1_path)\n",
    "    df2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Counts\n",
    "    count_1_ge1 = df1[df1['number_integrations'] >= 1].shape[0]\n",
    "    count_2_ge1 = df2[df2['number_integrations'] >= 1].shape[0]\n",
    "    df1_filt = df1[df1['number_integrations'] >= int_min]\n",
    "    df2_filt = df2[df2['number_integrations'] >= int_min]\n",
    "    count_1_ge5 = df1_filt.shape[0]\n",
    "    count_2_ge5 = df2_filt.shape[0]\n",
    "\n",
    "    # Merge on Mutation\n",
    "    merged = pd.merge(df1_filt[['Mutation', 'average_Ratio_Complex']],\n",
    "                      df2_filt[['Mutation', 'average_Ratio_Complex']],\n",
    "                      on='Mutation',\n",
    "                      suffixes=('_file1', '_file2'))\n",
    "\n",
    "    # Calculate ratio\n",
    "    merged['ratio'] = merged['average_Ratio_Complex_file2'] / merged['average_Ratio_Complex_file1']\n",
    "    merged = merged.set_index('Mutation').reindex([m for m in mutation_order if m in merged['Mutation'].values])\n",
    "\n",
    "\n",
    "    # --- Single continuous y-axis ---\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    colors = [mutation_colors.get(mut, 'gray') for mut in merged.index]\n",
    "\n",
    "    bars = ax.bar(merged.index, merged['ratio'], alpha=0.7, color=colors)\n",
    "\n",
    "    # Uniform y-axis across all plots\n",
    "    ax.set_ylim(0, 6.5)  # adjust upper bound to cover your data range\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_ylabel('File2 ÷ File1 Ratio')\n",
    "    ax.set_title(f'{file2_base} ÷ {file1_base} (Mutations with ≥{int_min} Integrations)')\n",
    "\n",
    "    plt.xticks(rotation=90, ha='center', fontsize=8)\n",
    "\n",
    "    # Add horizontal gridlines\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Annotate each bar with ratio\n",
    "   # for bar, ratio in zip(bars, merged['ratio']):\n",
    "       # ax.text(bar.get_x() + bar.get_width()/2, ratio + 0.1, f'{ratio:.2f}',\n",
    "                #ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "    # Add legend with counts\n",
    "    legend_text = (\n",
    "        f'{file1_base}: ≥1 ints = {count_1_ge1:,}, ≥{int_min} ints = {count_1_ge5:,}\\n'\n",
    "        f'{file2_base}: ≥1 ints = {count_2_ge1:,}, ≥{int_min} ints = {count_2_ge5:,}\\n'\n",
    "        f'Total mutations plotted: {merged.shape[0]:,}'\n",
    "    )\n",
    "    ax.legend([legend_text], loc='upper left', frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Extract df numbers for saving\n",
    "    df1_num = file1_base.split('_')[-1]\n",
    "    df2_num = file2_base.split('_')[-1]\n",
    "\n",
    "    # Save plot using only df numbers\n",
    "    plot_filename = f'ratio_plot_df_{df1_num}_vs_df_{df2_num}_{int_min}ints.png'\n",
    "    plot_path = os.path.join(out_dir8, plot_filename)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe2da8-a761-4840-a5a3-c9b8bffce93d",
   "metadata": {},
   "source": [
    "## Step 4 Closer Look at specific tiles / genes\n",
    "- hopefully you have lots of good tiles and looking at them all together is too crowded to use below code to identify and pull out specific tiles / genes/ whatever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a687c21-9f58-4bf1-8c9f-73700a594918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the top tiles within a df based on highest complex ratio, lowest spread, or lowest complex ratio\n",
    "#Good if you need to select a few mutants to put on a plot\n",
    "# Flexible function to compute top 10 mutations by chosen metric, requiring at least 5 replicates\n",
    "def get_top_mutations(df, label, metric=\"spread\", top_n=10, min_reps=5):\n",
    "    \"\"\"\n",
    "    metric: \"spread\" (lowest std), \"high\" (highest mean), \"low\" (lowest mean)\n",
    "    top_n: number of mutations to return\n",
    "    min_reps: minimum replicates required\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['Ratio_Complex'])\n",
    "    \n",
    "    # Count replicates per mutation\n",
    "    counts = df['Mutation'].value_counts()\n",
    "    valid_mutations = counts[counts >= min_reps].index\n",
    "    \n",
    "    # Filter to mutations with at least min_reps\n",
    "    df_filtered = df[df['Mutation'].isin(valid_mutations)]\n",
    "    \n",
    "    # Compute mean and std\n",
    "    stats = df_filtered.groupby('Mutation')['Ratio_Complex'].agg(['mean', 'std'])\n",
    "    \n",
    "    # Sort based on metric\n",
    "    if metric == \"spread\":\n",
    "        ranked = stats.sort_values(by='std', ascending=True).head(top_n)\n",
    "        print(f\"\\nTop {top_n} mutations with lowest spread in {label} (≥{min_reps} reps):\")\n",
    "    elif metric == \"high\":\n",
    "        ranked = stats.sort_values(by='mean', ascending=False).head(top_n)\n",
    "        print(f\"\\nTop {top_n} mutations with highest mean in {label} (≥{min_reps} reps):\")\n",
    "    elif metric == \"low\":\n",
    "        ranked = stats.sort_values(by='mean', ascending=True).head(top_n)\n",
    "        print(f\"\\nTop {top_n} mutations with lowest mean in {label} (≥{min_reps} reps):\")\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'spread', 'high', or 'low'\")\n",
    "    \n",
    "    display(ranked)\n",
    "    return ranked\n",
    "\n",
    "# Example loop over all dataframes\n",
    "for key, df in dataframes.items():\n",
    "    get_top_mutations(df, key, metric=\"spread\")   # lowest spread\n",
    "    #get_top_mutations(df, key, metric=\"high\")   # highest mean\n",
    "    #get_top_mutations(df, key, metric=\"low\")    # lowest mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc79cb-5ee0-49e3-a545-11d5b83c0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the top tiles within a df based on highest complex ratio, lowest spread, or lowest complex ratio\n",
    "# Collect top mutations from a single DataFrame\n",
    "def get_top_mutations(df, metric=\"spread\", n=20, min_reps=5):\n",
    "    \"\"\"\n",
    "    metric: \"spread\" (lowest std), \"highest\" (highest mean), \"lowest\" (lowest mean)\n",
    "    n: number of top mutations to return\n",
    "    min_reps: minimum replicates required\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['Ratio_Complex'])\n",
    "\n",
    "    # Count replicates per mutation\n",
    "    counts = df['Mutation'].value_counts()\n",
    "    valid_mutations = counts[counts >= min_reps].index\n",
    "\n",
    "    # Filter to mutations with at least min_reps\n",
    "    df_filtered = df[df['Mutation'].isin(valid_mutations)]\n",
    "\n",
    "    # Compute mean and std\n",
    "    stats = df_filtered.groupby('Mutation')['Ratio_Complex'].agg(['mean', 'std'])\n",
    "\n",
    "    # Decide sorting metric\n",
    "    if metric == \"spread\":\n",
    "        stats = stats.sort_values(by='std', ascending=True)  # lowest std\n",
    "    elif metric == \"highest\":\n",
    "        stats = stats.sort_values(by='mean', ascending=False)  # highest mean\n",
    "    elif metric == \"lowest\":\n",
    "        stats = stats.sort_values(by='mean', ascending=True)  # lowest mean\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'spread', 'highest', or 'lowest'\")\n",
    "\n",
    "    return stats.head(n)\n",
    "\n",
    "# General comparison function\n",
    "def compare_mutations(df_names, data_dict, metric=\"spread\", n=20, min_reps=5, top=10):\n",
    "    top_list = []\n",
    "    for name in df_names:\n",
    "        df = data_dict[name]\n",
    "        top_stats = get_top_mutations(df, metric=metric, n=n, min_reps=min_reps)\n",
    "        top_stats = top_stats[['mean','std']].assign(source=name)\n",
    "        top_list.append(top_stats)\n",
    "\n",
    "    combined = pd.concat(top_list)\n",
    "\n",
    "    # Count appearances and average values\n",
    "    appearance_counts = combined.groupby('Mutation').agg(\n",
    "        appearances=('source', 'count'),\n",
    "        avg_mean=('mean', 'mean'),\n",
    "        avg_spread=('std', 'mean')\n",
    "    )\n",
    "\n",
    "    # Sort depending on metric\n",
    "    if metric == \"spread\":\n",
    "        ranked = appearance_counts.sort_values(\n",
    "            by=['appearances', 'avg_spread'], ascending=[False, True]\n",
    "        ).head(top)\n",
    "    elif metric == \"highest\":\n",
    "        ranked = appearance_counts.sort_values(\n",
    "            by=['appearances', 'avg_mean'], ascending=[False, False]\n",
    "        ).head(top)\n",
    "    elif metric == \"lowest\":\n",
    "        ranked = appearance_counts.sort_values(\n",
    "            by=['appearances', 'avg_mean'], ascending=[False, True]\n",
    "        ).head(top)\n",
    "\n",
    "    print(f\"Top {top} mutations ({metric}) across {', '.join(df_names)}:\")\n",
    "    display(ranked)\n",
    "    return ranked\n",
    "\n",
    "# Example usage:\n",
    "df_names = ['df_1', 'df_2', 'df_5', 'df_6']  # keys from filtered_dataframes #this is all Rep1 example\n",
    "ranked_spread = compare_mutations(df_names, filtered_dataframes, metric=\"spread\")\n",
    "ranked_highest = compare_mutations(df_names, filtered_dataframes, metric=\"highest\")\n",
    "ranked_lowest = compare_mutations(df_names, filtered_dataframes, metric=\"lowest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcff03f-60a7-4681-b179-b55765e1f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot specific mutations on the same graph over various time points \n",
    "\n",
    "out_dir9 =  f\"{Lib_Name}_output_graphs/{Lib_Name}_output_graphs_mutations\"\n",
    "os.makedirs(out_dir9, exist_ok=True)\n",
    "\n",
    "# Define mutations to plot\n",
    "mut_identifier_name = 'muts_I_like' #name to add in the title and fig save naming to differentiate if you have mutliple diff graphs \n",
    "mutation_list = ['P > A',\n",
    "                  'P > L',\n",
    "                  'P > D',\n",
    "                  'VP16H1_killmotif1'\t\n",
    "                 ]  # Add any mutations you want\n",
    "\n",
    "# Define time mapping\n",
    "time_map = {\n",
    "    'df_3': 0,\n",
    "    'df_4': 24,\n",
    "    'df_8': 48\n",
    "}\n",
    "\n",
    "# Prepare plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Loop through each mutation\n",
    "for mutation_type in mutation_list:\n",
    "    mean_ratios = []\n",
    "    stderr_ratios = []\n",
    "    time_points = []\n",
    "\n",
    "    for df_name, time in time_map.items():\n",
    "        df = filtered_dataframes[df_name]\n",
    "        if 'Mutation' in df.columns and 'Ratio_Complex' in df.columns:\n",
    "            selected = df[df['Mutation'] == mutation_type]['Ratio_Complex'].dropna()\n",
    "            if len(selected) > 0:\n",
    "                mean_ratios.append(np.mean(selected))\n",
    "                stderr_ratios.append(np.std(selected, ddof=1) / np.sqrt(len(selected)))\n",
    "                time_points.append(time)\n",
    "\n",
    "    # Plot mean with error bars\n",
    "    plt.errorbar(time_points, mean_ratios, yerr=stderr_ratios, fmt='o-', capsize=5, label=mutation_type)\n",
    "\n",
    "# Final formatting\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Ratio Complex')\n",
    "plt.title(f'Mean Ratio Complex Over Time by Mutation for {mut_identifier_name}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plot_filename2 = f'{Lib_Name}_{mut_identifier_name}_muts_over_time.jpg'\n",
    "plot_pathnew = os.path.join(out_dir9, plot_filename2)\n",
    "plt.savefig(plot_pathnew, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32860b20-a73b-4bf5-9657-71a6d924442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot multiple mutations on the same plot at the same time \n",
    "# Define mutations to plot\n",
    "timepoint = '24_Hours'\n",
    "mutations_name = 'good_looking_muts' #name to add in the title and fig save naming to differentiate if you have mutliple diff graphs \n",
    "mutation_list = ['P > A',\n",
    "                 'P > L',\n",
    "                 'P > D', \n",
    "                 'T > A',\n",
    "                 'Q > T',\n",
    "                 'VP16H1_killmotif1',\n",
    "                 'VP16C_Y2A']\n",
    "\n",
    "# Select the 24-hour DataFrame\n",
    "df_24 = filtered_dataframes['df_4']\n",
    "\n",
    "# Prepare plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Track max y-value to position sample counts\n",
    "global_max = 0\n",
    "sample_counts = []\n",
    "\n",
    "# Loop through each mutation\n",
    "for i, mutation_type in enumerate(mutation_list):\n",
    "    if 'Mutation' in df_24.columns and 'Ratio_Complex' in df_24.columns:\n",
    "        selected = df_24[df_24['Mutation'] == mutation_type]['Ratio_Complex'].dropna()\n",
    "\n",
    "        # Plot individual dots\n",
    "        x_vals = np.full(len(selected), i)\n",
    "        plt.scatter(x_vals, selected, color='gray', alpha=0.6, s=40)\n",
    "\n",
    "        # Plot mean ± SEM\n",
    "        if len(selected) > 0:\n",
    "            mean = np.mean(selected)\n",
    "            sem = np.std(selected, ddof=1) / np.sqrt(len(selected))\n",
    "            plt.errorbar(i, mean, yerr=sem, fmt='o', color='black', capsize=5, markersize=8)\n",
    "\n",
    "            global_max = max(global_max, max(selected) + sem)\n",
    "            sample_counts.append((i, len(selected)))\n",
    "\n",
    "# Add sample counts at top of plot\n",
    "y_top = global_max + 0.1 * global_max\n",
    "for i, count in sample_counts:\n",
    "    plt.text(i, y_top, f'n={count}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Final formatting\n",
    "plt.xticks(range(len(mutation_list)), mutation_list, rotation=45)\n",
    "plt.ylabel('TREBL-Seq Activity')\n",
    "plt.xlabel('Mutation')\n",
    "plt.title(f'TREBL-Seq Activity at {timepoint} for {mutations_name}')\n",
    "plt.ylim(0, y_top + 0.1 * y_top)\n",
    "plt.tight_layout()\n",
    "plot_filename2 = f'{Lib_Name}_{mutations_name}_muts_{timepoint}.jpg'\n",
    "plot_pathnew = os.path.join(out_dir9, plot_filename2)\n",
    "plt.savefig(plot_pathnew, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950417a-bf4c-4819-9443-fb82300d7b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
